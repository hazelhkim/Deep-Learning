# Backpropagation

Another method of training a neural network.
  - Doing a feedforward operation.
  - Comparing the output of the model with the desired output.
  - Calculating the error.
  - Running the feedforward operation backwards (backpropagation) to spread the error to each of the weights.
  - Use this to update the weights, and get a better model.
  - Continue this until we have a model that is good.
  
  This ( the point x1, x2 ) is bad perceptron, since the point is blue in the red region.
  <img width="1568" alt="Screen Shot 2019-09-25 at 12 29 02 AM" src="https://user-images.githubusercontent.com/46575719/65569406-83d6f000-df2b-11e9-8080-f1de80e406f7.png">
  
  What we did in the Gradient Descent Algorithm. We did this thing called **Backpropagation**. We went in the opposite direction.
  - The point would say to us, "I am misclassified so I want this model to come closer to me."
  - And we saw that the line got closer to it by updating the weights. 
  
  ![Screen Shot 2019-09-25 at 12 33 16 AM](https://user-images.githubusercontent.com/46575719/65569561-17a8bc00-df2c-11e9-823d-24ea474356b6.png)
  
  - It tells the weight w1 to go lower and the weight w2 to go higher, for example.
  - So we obtain new weights, w1' and w2' which define a new line which is now closer to the point.
  
  <img width="1556" alt="Screen Shot 2019-09-25 at 12 41 31 AM" src="https://user-images.githubusercontent.com/46575719/65569902-42474480-df2d-11e9-82f1-32ac8b298962.png">

  - The height is going to be the error function E(W) and we calculate the gradient of the error funciton which is exactly like asking the point what does is it want the model to do.
  And as we take the step down the direction of the negative of the gradient, we decrease the error to come down the mountain. 
  - This gives us a new error E(W') and a new model W' with a smaller error, which means we get a new line closer to the point. 
  - We continue doing this process in order to minimize the error. 
  - And this was for single perceptron
  
  #### Now, what about Multi-Layer Perceptrons?
  We still do the same process of reducing the error by descending from the mountain, except the error function is more complicated.
  
  <img width="727" alt="Screen Shot 2019-09-25 at 12 58 17 AM" src="https://user-images.githubusercontent.com/46575719/65570577-97845580-df2f-11e9-9a44-9bdfb3bef8d3.png">
  
  - Calculate the error function and its gradient.
  - Then walk in the direction of the negative of the gradient in order to find a new model W' with a smaller error E(W') which will give us a better prediction. 
  - Continue doing this process in order to minimize the error. 
  
  
<img width="718" alt="Screen Shot 2019-09-25 at 1 00 35 AM" src="https://user-images.githubusercontent.com/46575719/65570675-ea5e0d00-df2f-11e9-8fed-d813c6d6c722.png">

  - After we update all the weights, we have better predictions at all the models in the hidden layer and also a better prediction at the model in the output layer.
  - When you update the weights, we're also updating the bias unit.
  
  #### Backpropagation Math
  
  <img width="743" alt="Screen Shot 2019-09-25 at 1 08 25 AM" src="https://user-images.githubusercontent.com/46575719/65570980-057d4c80-df31-11e9-9c6a-865e0e30c184.png">
  
  - On the left image, you have a single perceptron with the input vector, the weights and the bias and the sigmoid function inside the node. 
  - **Prediction** : the sigmoid function of the linear function of the input.
  - **Error Function** : the average of all points of the blue term for the blue points and the red term for the red points.
  - **Gradient** : the vector formed by all the partial derivatives of the error function with respect to the weights w1 up to wn and the bias b. 
  
  
#### For the multi-layer perceptron
  
  
<img width="749" alt="Screen Shot 2019-09-25 at 1 09 55 AM" src="https://user-images.githubusercontent.com/46575719/65571033-33629100-df31-11e9-92b1-919fbe0dc717.png">
<img width="742" alt="Screen Shot 2019-09-25 at 1 10 41 AM" src="https://user-images.githubusercontent.com/46575719/65571069-4ecd9c00-df31-11e9-83ad-5a513933e406.png">
  
  
#### Feedforward

<img width="751" alt="Screen Shot 2019-09-25 at 1 12 55 AM" src="https://user-images.githubusercontent.com/46575719/65571173-bedc2200-df31-11e9-996a-83a78eec2684.png">

Feedfoward = is to get a prediction ( y_hat ) = by the sigmoid of W^(2) combined with the sigmoid of W^(1) applied to the input x.

#### Backpropagation

= the reverse of feedforward.
- Calculate the derivative of this error function with respect to each of the weights in the labels by using the chain rule.

<img width="746" alt="Screen Shot 2019-09-25 at 1 26 09 AM" src="https://user-images.githubusercontent.com/46575719/65571652-79b8ef80-df33-11e9-802a-8d4a80ce7111.png">

<img width="745" alt="Screen Shot 2019-09-25 at 1 27 16 AM" src="https://user-images.githubusercontent.com/46575719/65571698-a1a85300-df33-11e9-9f96-192c156d8208.png">

<img width="741" alt="Screen Shot 2019-09-25 at 1 27 42 AM" src="https://user-images.githubusercontent.com/46575719/65571717-b08f0580-df33-11e9-84e8-c11775701855.png">




