# Implementing Backpropagation

## Backpropagation
Now we've come to the problem of how to make a multilayer neural network learn. Before, we saw how to update weights with gradient descent. The backpropagation algorithm is just an extension of that, using the chain rule to find the error with the respect to the weights connecting the input layer to the hidden layer (for a two layer network).
<br />

To update the weights to hidden layers using gradient descent, you need to know how much error each of the hidden units contributed to the final output. Since the output of a layer is determined by the weights between layers, the error resulting from units is scaled by the weights going forward through the network. Since we know the error at the output, we can use the weights to work backwards to hidden layers.




Working through an example

From this example, you can see one of the effects of using the sigmoid function for the activations. 
The maximum derivative of the sigmoid function is 0.25, so the errors in the output layer get reduced by at least 75%, and errors in the hidden layer are scaled down by at least 93.75%! 
You can see that if you have a lot of layers, using a sigmoid activation function will quickly reduce the weight steps to tiny values in layers near the input. 
This is known as the vanishing gradient problem. 
Later in the course you'll learn about other activation functions that perform better in this regard and are more commonly used in modern network architectures.


#### Implementing in Numpy

For the most part you have everything you need to implement backpropagation with NumPy.


<img width="771" alt="Screen Shot 2019-09-25 at 9 19 35 PM" src="https://user-images.githubusercontent.com/46575719/65650776-31084180-dfda-11e9-8003-000f51addcf9.png">

1. Firstly, there will likely be a different number of input and hidden units, so trying to multiply the errors and the inputs as row vectors will throw an error.
2. Also, Wij is a matrix now, so the right side of the assignment must have the same shape as the left side. Luckily, NumPy takes care of this for us. If you multiply a row vector array with a column vector array, it will multiply the first element in the column by each element in the row vector and set that as the first row in a new 2D array. This continues for each element in the column vector, so you get a 2D array that has shape (len(column_vector), len(row_vector)).

<img width="781" alt="Screen Shot 2019-09-25 at 9 20 40 PM" src="https://user-images.githubusercontent.com/46575719/65650823-5a28d200-dfda-11e9-9ff9-ef361f591b03.png">

## Calculating Backpropagation
1. Calculate the network's output error.
2. Calculate the output layer's error term.
3. Use backpropagation to calculate the hidden layer's error term.
4. Calculate the change in weights (the delta weights) that result from propagating the errors back through the network.

