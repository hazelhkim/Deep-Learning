# Implementing Gradient Descent

## Gradient Descent

Gradient is another term for rate of change or slope.

<img width="801" alt="Screen Shot 2019-09-25 at 1 54 15 AM" src="https://user-images.githubusercontent.com/46575719/65572903-850e1a00-df37-11e9-8b13-65296425004d.png">
<img width="816" alt="Screen Shot 2019-09-25 at 1 54 27 AM" src="https://user-images.githubusercontent.com/46575719/65572907-87707400-df37-11e9-84a1-ef758eb1bfc2.png">
<img width="815" alt="Screen Shot 2019-09-25 at 1 54 53 AM" src="https://user-images.githubusercontent.com/46575719/65572909-88a1a100-df37-11e9-8589-a2ff6b426dad.png">

https://distill.pub/2017/momentum/

<img width="823" alt="Screen Shot 2019-09-25 at 1 55 57 AM" src="https://user-images.githubusercontent.com/46575719/65638057-5a61a700-dfb3-11e9-83d2-7d2dd37a54c5.png">
<img width="762" alt="Screen Shot 2019-09-25 at 1 57 38 AM" src="https://user-images.githubusercontent.com/46575719/65638059-5afa3d80-dfb3-11e9-82e6-d410992a1d80.png">
<img width="784" alt="Screen Shot 2019-09-25 at 1 58 12 AM" src="https://user-images.githubusercontent.com/46575719/65638063-5c2b6a80-dfb3-11e9-96c6-de022dc57fb2.png">
<img width="661" alt="Screen Shot 2019-09-25 at 1 59 06 AM" src="https://user-images.githubusercontent.com/46575719/65638066-5cc40100-dfb3-11e9-8d2d-562ef9b8e445.png">
<img width="721" alt="Screen Shot 2019-09-25 at 2 00 40 AM" src="https://user-images.githubusercontent.com/46575719/65638072-5e8dc480-dfb3-11e9-9c8b-56b441ef5550.png">
<img width="660" alt="Screen Shot 2019-09-25 at 2 01 58 AM" src="https://user-images.githubusercontent.com/46575719/65638078-60f01e80-dfb3-11e9-9319-4fcdbbd951ec.png">
<img width="553" alt="Screen Shot 2019-09-25 at 2 02 16 AM" src="https://user-images.githubusercontent.com/46575719/65638098-6483a580-dfb3-11e9-8828-7f6b0381e990.png">


## The Code

<img width="815" alt="Screen Shot 2019-09-25 at 4 39 45 PM" src="https://user-images.githubusercontent.com/46575719/65638123-6fd6d100-dfb3-11e9-9d32-4c13fe3652e3.png">

```python

# Defining the sigmoid function for activations
def sigmoid(x):
    return 1/(1+np.exp(-x))

# Derivative of the sigmoid function
def sigmoid_prime(x):
    return sigmoid(x) * (1 - sigmoid(x))

# Input data
x = np.array([0.1, 0.3])
# Target
y = 0.2
# Input to output weights
weights = np.array([-0.8, 0.5])

# The learning rate, eta in the weight step equation
learnrate = 0.5

# the linear combination performed by the node (h in f(h) and f'(h))
h = x[0]*weights[0] + x[1]*weights[1]
# or h = np.dot(x, weights)

# The neural network output (y-hat)
nn_output = sigmoid(h)

# output error (y - y-hat)
error = y - nn_output

# output gradient (f'(h))
output_grad = sigmoid_prime(h)

# error term (lowercase delta)
error_term = error * output_grad

# Gradient descent step 
del_w = [ learnrate * error_term * x[0],
          learnrate * error_term * x[1]]
# or del_w = learnrate * error_term * x

```



```python
import numpy as np

def sigmoid(x):
    """
    Calculate sigmoid
    """
    return 1/(1+np.exp(-x))

def sigmoid_prime(x):
    """
    # Derivative of the sigmoid function
    """
    return sigmoid(x) * (1 - sigmoid(x))

learnrate = 0.5
x = np.array([1, 2. 3, 4])
y = np.array(0.5)

# Initial weights
w = np.array([0.5, -0.5, 0.3, 0.1])

### Calculate one gradient descent step for each weight
### Note: Some steps have been consolidated, so there are
###       fewer variable names than in the above sample code

# TODO: Calculate the node's linear combination of inputs and weights
h = np.dot(x, w)

# TODO: Calculate output of neural network
nn_output = sigmoid(h)

# TODO: Calculate error of neural network
error = y - nn_output

# TODO: Calculate the error term
#       Remember, this requires the output gradient, which we haven't
#       specifically added a variable for.
error_term = error * sigmoid_prime(h)
# Note: The sigmoid_prime function calculates sigmoid(h) twice,
#       but you've already calculated it once. You can make this
#       code more efficient by calculating the derivative directly
#       rather than calling sigmoid_prime, like this:
# error_term = error * nn_output * (1 - nn_output)

# TODO: Calculate change in weights
del_w = learnrate * error_term * x

print('Neural Network output:')
print(nn_output)
print('Amount of Error:')
print(error)
print('Change in Weights:')
print(del_w)

```


