# Implementing Gradient Descent

## Gradient Descent

Gradient is another term for rate of change or slope.

<img width="801" alt="Screen Shot 2019-09-25 at 1 54 15 AM" src="https://user-images.githubusercontent.com/46575719/65572903-850e1a00-df37-11e9-8b13-65296425004d.png">
<img width="816" alt="Screen Shot 2019-09-25 at 1 54 27 AM" src="https://user-images.githubusercontent.com/46575719/65572907-87707400-df37-11e9-84a1-ef758eb1bfc2.png">
<img width="815" alt="Screen Shot 2019-09-25 at 1 54 53 AM" src="https://user-images.githubusercontent.com/46575719/65572909-88a1a100-df37-11e9-8589-a2ff6b426dad.png">

https://distill.pub/2017/momentum/

<img width="823" alt="Screen Shot 2019-09-25 at 1 55 57 AM" src="https://user-images.githubusercontent.com/46575719/65638057-5a61a700-dfb3-11e9-83d2-7d2dd37a54c5.png">
<img width="762" alt="Screen Shot 2019-09-25 at 1 57 38 AM" src="https://user-images.githubusercontent.com/46575719/65638059-5afa3d80-dfb3-11e9-82e6-d410992a1d80.png">
<img width="784" alt="Screen Shot 2019-09-25 at 1 58 12 AM" src="https://user-images.githubusercontent.com/46575719/65638063-5c2b6a80-dfb3-11e9-96c6-de022dc57fb2.png">
<img width="661" alt="Screen Shot 2019-09-25 at 1 59 06 AM" src="https://user-images.githubusercontent.com/46575719/65638066-5cc40100-dfb3-11e9-8d2d-562ef9b8e445.png">
<img width="721" alt="Screen Shot 2019-09-25 at 2 00 40 AM" src="https://user-images.githubusercontent.com/46575719/65638072-5e8dc480-dfb3-11e9-9c8b-56b441ef5550.png">
<img width="660" alt="Screen Shot 2019-09-25 at 2 01 58 AM" src="https://user-images.githubusercontent.com/46575719/65638078-60f01e80-dfb3-11e9-9319-4fcdbbd951ec.png">
<img width="553" alt="Screen Shot 2019-09-25 at 2 02 16 AM" src="https://user-images.githubusercontent.com/46575719/65638098-6483a580-dfb3-11e9-8828-7f6b0381e990.png">


## The Code

<img width="815" alt="Screen Shot 2019-09-25 at 4 39 45 PM" src="https://user-images.githubusercontent.com/46575719/65638123-6fd6d100-dfb3-11e9-9d32-4c13fe3652e3.png">

```python

# Defining the sigmoid function for activations
def sigmoid(x):
    return 1/(1+np.exp(-x))

# Derivative of the sigmoid function
def sigmoid_prime(x):
    return sigmoid(x) * (1 - sigmoid(x))

# Input data
x = np.array([0.1, 0.3])
# Target
y = 0.2
# Input to output weights
weights = np.array([-0.8, 0.5])

# The learning rate, eta in the weight step equation
learnrate = 0.5

# the linear combination performed by the node (h in f(h) and f'(h))
h = x[0]*weights[0] + x[1]*weights[1]
# or h = np.dot(x, weights)

# The neural network output (y-hat)
nn_output = sigmoid(h)

# output error (y - y-hat)
error = y - nn_output

# output gradient (f'(h))
output_grad = sigmoid_prime(h)

# error term (lowercase delta)
error_term = error * output_grad

# Gradient descent step 
del_w = [ learnrate * error_term * x[0],
          learnrate * error_term * x[1]]
# or del_w = learnrate * error_term * x

```



```python
import numpy as np

def sigmoid(x):
    """
    Calculate sigmoid
    """
    return 1/(1+np.exp(-x))

def sigmoid_prime(x):
    """
    # Derivative of the sigmoid function
    """
    return sigmoid(x) * (1 - sigmoid(x))

learnrate = 0.5
x = np.array([1, 2. 3, 4])
y = np.array(0.5)

# Initial weights
w = np.array([0.5, -0.5, 0.3, 0.1])

### Calculate one gradient descent step for each weight
### Note: Some steps have been consolidated, so there are
###       fewer variable names than in the above sample code

# TODO: Calculate the node's linear combination of inputs and weights
h = np.dot(x, w)

# TODO: Calculate output of neural network
nn_output = sigmoid(h)

# TODO: Calculate error of neural network
error = y - nn_output

# TODO: Calculate the error term
#       Remember, this requires the output gradient, which we haven't
#       specifically added a variable for.
error_term = error * sigmoid_prime(h)
# Note: The sigmoid_prime function calculates sigmoid(h) twice,
#       but you've already calculated it once. You can make this
#       code more efficient by calculating the derivative directly
#       rather than calling sigmoid_prime, like this:
# error_term = error * nn_output * (1 - nn_output)

# TODO: Calculate change in weights
del_w = learnrate * error_term * x

print('Neural Network output:')
print(nn_output)
print('Amount of Error:')
print(error)
print('Change in Weights:')
print(del_w)

```


## Implementing Gradient Descent

<img width="425" alt="Screen Shot 2019-09-25 at 5 55 34 PM" src="https://user-images.githubusercontent.com/46575719/65642625-c47f4980-dfbd-11e9-9054-2997580f61a8.png">
<img width="839" alt="Screen Shot 2019-09-25 at 5 55 28 PM" src="https://user-images.githubusercontent.com/46575719/65642627-c5b07680-dfbd-11e9-99b9-485e6e6c700e.png">

```python


import numpy as np
from data_prep import features, targets, features_test, targets_test


def sigmoid(x):
    """
    Calculate sigmoid
    """
    return 1 / (1 + np.exp(-x))

# TODO: We haven't provided the sigmoid_prime function like we did in
#       the previous lesson to encourage you to come up with a more
#       efficient solution. If you need a hint, check out the comments
#       in solution.py from the previous lecture.

# Use to same seed to make debugging easier
np.random.seed(42)

n_records, n_features = features.shape
last_loss = None

# Initialize weights
weights = np.random.normal(scale=1 / n_features**.5, size=n_features)

# Neural Network hyperparameters
epochs = 1000
learnrate = 0.5

for e in range(epochs):
    del_w = np.zeros(weights.shape)
    for x, y in zip(features.values, targets):
        # Loop through all records, x is the input, y is the target

        # Activation of the output unit
        #   Notice we multiply the inputs and the weights here 
        #   rather than storing h as a separate variable 
        output = sigmoid(np.dot(x, weights))

        # The error, the target minus the network output
        error = y - output

        # The error term
        #   Notice we calulate f'(h) here instead of defining a separate
        #   sigmoid_prime function. This just makes it faster because we
        #   can re-use the result of the sigmoid function stored in
        #   the output variable
        error_term = error * output * (1 - output)

        # The gradient descent step, the error times the gradient times the inputs
        del_w += error_term * x

    # Update the weights here. The learning rate times the 
    # change in weights, divided by the number of records to average
    weights += learnrate * del_w / n_records

    # Printing out the mean square error on the training set
    if e % (epochs / 10) == 0:
        out = sigmoid(np.dot(features, weights))
        loss = np.mean((out - targets) ** 2)
        if last_loss and last_loss < loss:
            print("Train loss: ", loss, "  WARNING - Loss Increasing")
        else:
            print("Train loss: ", loss)
        last_loss = loss


# Calculate accuracy on test data
tes_out = sigmoid(np.dot(features_test, weights))
predictions = tes_out > 0.5
accuracy = np.mean(predictions == targets_test)
print("Prediction accuracy: {:.3f}".format(accuracy))


```
